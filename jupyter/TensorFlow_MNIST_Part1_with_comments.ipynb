{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TFCourse/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFLow includes a data provider for MNIST that we'll use.\n",
    "# It comes with the tensorflow-datasets module, therefore, if you haven't please install the package using\n",
    "# pip install tensorflow-datasets \n",
    "# or\n",
    "# conda install tensorflow-datasets\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "That's where we load and preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the comment from above\n",
    "# these datasets will be stored in C:\\Users\\*USERNAME*\\tensorflow_datasets\\...\n",
    "# the first time you download a dataset, it is stored in the respective folder \n",
    "# every other time, it is automatically loading the copy on your computer \n",
    "\n",
    "# tfds.load actually loads a dataset (or downloads and then loads if that's the first time you use it) \n",
    "# in our case, we are interesteed in the MNIST; the name of the dataset is the only mandatory argument\n",
    "# there are other arguments we can specify, which we can find useful\n",
    "# mnist_dataset = tfds.load(name='mnist', as_supervised=True)\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "# with_info=True will also provide us with a tuple containing information about the version, features, number of samples\n",
    "# we will use this information a bit below and we will store it in mnist_info\n",
    "\n",
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) \n",
    "# alternatively, as_supervised=False, would return a dictionary\n",
    "# obviously we prefer to have our inputs and targets separated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>,\n",
       " 'train': <_PrefetchDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='/Users/joaogouveiaml/tensorflow_datasets/mnist/3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 06:59:39.380021: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:387] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-05-16 06:59:39.534773: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/TFCourse/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape = (28, 28, 1)), # transforms each image in a flat vector\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # find the dot product of inputs and weights and add bias\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # softmax as we want to tranform the output into probabilities of being one of the numbers on dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 1s - 2ms/step - accuracy: 0.9060 - loss: 0.3350 - val_accuracy: 0.9480 - val_loss: 0.1852\n",
      "Epoch 2/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9578 - loss: 0.1399 - val_accuracy: 0.9632 - val_loss: 0.1290\n",
      "Epoch 3/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9700 - loss: 0.0989 - val_accuracy: 0.9707 - val_loss: 0.0999\n",
      "Epoch 4/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9774 - loss: 0.0755 - val_accuracy: 0.9770 - val_loss: 0.0782\n",
      "Epoch 5/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9818 - loss: 0.0597 - val_accuracy: 0.9818 - val_loss: 0.0685\n",
      "Epoch 6/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9850 - loss: 0.0491 - val_accuracy: 0.9837 - val_loss: 0.0626\n",
      "Epoch 7/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9874 - loss: 0.0403 - val_accuracy: 0.9862 - val_loss: 0.0535\n",
      "Epoch 8/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9891 - loss: 0.0347 - val_accuracy: 0.9857 - val_loss: 0.0491\n",
      "Epoch 9/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9915 - loss: 0.0263 - val_accuracy: 0.9872 - val_loss: 0.0419\n",
      "Epoch 10/10\n",
      "540/540 - 1s - 1ms/step - accuracy: 0.9919 - loss: 0.0259 - val_accuracy: 0.9832 - val_loss: 0.0552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3078c48c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "model.fit(train_data, epochs = NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose = 2, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9758 - loss: 0.0808\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.58% \n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}% '.format(test_loss, test_accuracy * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageOps\n",
    "from PIL import ImageEnhance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts this is a 2\n"
     ]
    }
   ],
   "source": [
    "# Função para carregar e preparar a imagem\n",
    "def load_and_prepare_image(image_path):\n",
    "    img = Image.open(image_path).convert('L')\n",
    "\n",
    "    # Aplicar autocontraste\n",
    "    img = ImageOps.autocontrast(img)\n",
    "\n",
    "    # Verifique visualmente se precisa inverter\n",
    "    img = ImageOps.invert(img)  # Remova se sua imagem já for branco no preto\n",
    "\n",
    "    # Reduzir mantendo a proporção\n",
    "    img.thumbnail((28, 28), Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Normalizar e expandir dims\n",
    "    img_array = np.array(img).astype('float32') / 255.0\n",
    "    img_array = np.expand_dims(img, axis=(0, -1))  # (1, 28, 28, 1)\n",
    "\n",
    "    return img_array\n",
    "\n",
    "# Caminho da sua imagem\n",
    "image_path = 'my2_2.png'\n",
    "\n",
    "# Carregar e preparar a imagem\n",
    "img_array = load_and_prepare_image(image_path)\n",
    "\n",
    "# Fazer a previsão\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Obter a classe com maior probabilidade\n",
    "predicted_class = np.argmax(predictions)\n",
    "\n",
    "# Exibir a imagem e o resultado\n",
    "plt.imshow(img_array[0], cmap='gray')\n",
    "plt.title(f'Predicted Class: {predicted_class}')\n",
    "plt.show()\n",
    "\n",
    "print(f'The model predicts this is a {predicted_class}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting model to tf lite to use on Android app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/y2/th9j6g7n1wgghjg95sgj0cqr0000gq/T/tmp_avx5z7_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/y2/th9j6g7n1wgghjg95sgj0cqr0000gq/T/tmp_avx5z7_/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/y2/th9j6g7n1wgghjg95sgj0cqr0000gq/T/tmp_avx5z7_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  12997942480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12997943248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12997943056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12997943824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12997943632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  12997942672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1747375289.040642   24185 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1747375289.040653   24185 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-05-16 07:01:29.040851: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/y2/th9j6g7n1wgghjg95sgj0cqr0000gq/T/tmp_avx5z7_\n",
      "2025-05-16 07:01:29.041116: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-05-16 07:01:29.041120: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/y2/th9j6g7n1wgghjg95sgj0cqr0000gq/T/tmp_avx5z7_\n",
      "I0000 00:00:1747375289.042590   24185 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "2025-05-16 07:01:29.042823: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-05-16 07:01:29.051498: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/y2/th9j6g7n1wgghjg95sgj0cqr0000gq/T/tmp_avx5z7_\n",
      "2025-05-16 07:01:29.054425: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 13576 microseconds.\n",
      "2025-05-16 07:01:29.060086: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "# Save the model.\n",
    "with open('mnist-jg.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TFCourse]",
   "language": "python",
   "name": "conda-env-TFCourse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
